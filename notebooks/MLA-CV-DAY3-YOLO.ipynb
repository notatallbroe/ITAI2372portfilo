{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjPVaAZZ7fvs"
      },
      "source": [
        "![MLU Logo](https://github.com/aws-samples/aws-machine-learning-university-accelerated-cv/blob/main/data/MLU_Logo.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG3RC7fl7fvt"
      },
      "source": [
        "# <a name=\"0\">Machine Learning Accelerator - Computer Vision - Lecture 3</a>\n",
        "\n",
        "\n",
        "## Object Detection with YOLO\n",
        "\n",
        "In this notebook, we use pre-trained YOLO models for object detection task, with only a few\n",
        "lines of code. Here are the topics for this notebook:\n",
        "\n",
        "1. <a href=\"#1\">Downloading a Pretrained Model</a>      \n",
        "2. <a href=\"#2\">Preprocessing an Image</a>\n",
        "3. <a href=\"#3\">Inference and Visualizating</a>\n",
        "4. <a href=\"#4\">Putting it all Together</a>\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [],
        "id": "hyB5tFqp7fvu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install -q -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [],
        "id": "WItOncfO7fvu"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "# Suppress all warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [],
        "id": "pK7bvM3R7fvv"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwtlPOBU7fvv"
      },
      "source": [
        "## 1. <a name=\"1\">Downloading a Pretrained Model</a>\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "Let's get an YOLOv5 model as the base model. You will use 'yolov5s' in the following example which is the lightest and fastest YOLOv5 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [],
        "id": "O21xKdiD7fvv"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "net = torch.hub.load('ultralytics/yolov5', 'yolov5s', trust_repo=True, verbose=False, device='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTvCmI0g7fvw"
      },
      "source": [
        "## 2. <a name=\"2\">Pre-processing an Image</a>\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "Next we download an image for inference. You can\n",
        "feed an arbitrarily sized image, except one constraint for\n",
        "YOLO is that input height and width can be divided by 32.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohSy3CBd-8LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "tags": [],
        "id": "vG43oScH7fvw"
      },
      "outputs": [],
      "source": [
        "cat_dog = Image.open('PearBaby.webp')\n",
        "cat_dog.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF7xYMq17fvx"
      },
      "source": [
        "To predict objects shown on the image, wrap the image in `net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "tags": [],
        "id": "ia2CYPoq7fvx",
        "outputId": "d1bb5350-d680-470c-a2ec-9531424cd7b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "YOLOv5 <class 'models.common.Detections'> instance\n",
              "image 1/1: 1024x1024 (no detections)\n",
              "Speed: 45.4ms pre-process, 546.5ms inference, 25.0ms NMS per image at shape (1, 3, 640, 640)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "result = net(cat_dog)\n",
        "\n",
        "# Show the result\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat_dog = Image.open('honse1.jpeg')\n",
        "cat_dog.show()"
      ],
      "metadata": {
        "id": "Po3cTlHs_ony",
        "outputId": "790e0cd3-bd9c-4882-cac5-164b408fbea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'honse1.jpeg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-6bae4a44e3fa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcat_dog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'honse1.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcat_dog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3465\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3466\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3467\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'honse1.jpeg'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVImS9_z7fvx"
      },
      "source": [
        "## 3. <a name=\"3\">Inference and Visualizating</a>\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "The forward function, i.e. `net(x)` will return all detected bounding boxes, and the\n",
        "corresponding predicted class names. To see the bounding boxes you can call `.pandas().xyxy[0]` on the result. You can also check which classes are part of the model by running `net.names`. The shape can be shown with `net(cat_dog).s`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": [],
        "id": "6p67BGB97fvy",
        "outputId": "649ce258-63c9-4916-832e-550a05e427ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'result' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-35a13bf4235c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxyxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
          ]
        }
      ],
      "source": [
        "result.pandas().xyxy[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhBS2ykK7fvy"
      },
      "source": [
        "To visualize the bounding boxes on the image itself, you can use `result.show()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "in2f3Uh37fvy"
      },
      "outputs": [],
      "source": [
        "result.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jucTifPH7fvy"
      },
      "source": [
        "## 4. <a name=\"4\">Putting it all Together</a>\n",
        "(<a href=\"#0\">Go to top</a>)\n",
        "\n",
        "Let's load the first example and add another image to verify the stability of YOLO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sumt2FZQ7fvy"
      },
      "outputs": [],
      "source": [
        "cat_dog = '../data/catdog.png'\n",
        "dog_bike = '../data/bikedog.jpg'\n",
        "\n",
        "# Inference - pass in as many images as you like here\n",
        "results = net([cat_dog, dog_bike], size=640) # batch of images\n",
        "\n",
        "# Results\n",
        "results.print()\n",
        "results.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4bShSw17fvz"
      },
      "source": [
        "If you wanted to access the probability values for the predictions in the second image, you have to increase the index of `xyxy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wtVwNXK07fvz"
      },
      "outputs": [],
      "source": [
        "# Show values for second image\n",
        "for obj in results.xyxy[0]:\n",
        "    x1, y1, x2, y2, conf, cls = obj\n",
        "    print(f\"Class: {net.names[int(cls)]}, Confidence: {conf}, Probability: {obj[5:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "zv8GWXQj7fvz"
      },
      "outputs": [],
      "source": [
        "# Show values for second image\n",
        "for obj in results.xyxy[1]:\n",
        "    x1, y1, x2, y2, conf, cls = obj\n",
        "    print(f\"Class: {net.names[int(cls)]}, Confidence: {conf}, Probability: {obj[5:]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeuCIX9I7fvz"
      },
      "source": [
        "Is the YOLO model inference simple and straightforward? If you want to learn object detection techniques, feel free to check more techniques at D2L [Computer Vision chapter](https://d2l.ai/chapter_computer-vision/index.html)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "sagemaker-distribution:Python",
      "language": "python",
      "name": "conda-env-sagemaker-distribution-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}